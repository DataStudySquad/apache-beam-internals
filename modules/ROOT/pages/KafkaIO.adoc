= KafkaIO

`KafkaIO` is a utility to create xref:PTransform.adoc[]s for <<read, reading>> and <<write, writing>> records from Kafka topics.

== [[read]] Reading Kafka Records -- read Utility

[source,java]
----
Read<K, V> read()
----

read creates a xref:KafkaIO-Read.adoc[] root PTransform (that produces a xref:PCollection.adoc[] of KafkaRecords of K-keys and V-values).

== [[readBytes]] Reading Kafka Records -- readBytes Utility

[source,java]
----
Read<byte[], byte[]> readBytes()
----

readBytes creates a xref:KafkaIO-Read.adoc[] root PTransform (that produces a xref:PCollection.adoc[] of KafkaRecords of `byte[]` keys and values).

readBytes is a specialized <<read, read>> with ByteArrayDeserializer for the keys and values.

== [[write]] Writing Records to Kafka Topic -- write Utility

[source,java]
----
Write<K, V> write()
----

write creates a xref:KafkaIO-Write.adoc[] output PTransform.

== [[writeRecords]] Writing Records to Kafka Topic -- writeRecords Utility

[source,java]
----
WriteRecords<K, V> writeRecords()
----

writeRecords creates a xref:PTransform.adoc#sink[sink PTransform] (`PTransform<PCollection<ProducerRecord<K, V>>, PDone>`).

== [[demo]] Demo

[source,plaintext]
----
import org.apache.beam.sdk.io.kafka.KafkaIO
import org.apache.kafka.common.serialization.Serdes
val records = KafkaIO
  .read[String, String]()
  .withBootstrapServers(":9092")
  .withTopic("beam-input")
  .withKeyDeserializer(Serdes.String.deserializer.getClass)
  .withValueDeserializer(Serdes.String.deserializer.getClass)
import org.apache.beam.sdk.io.kafka.KafkaIO.Read
assert(records.isInstanceOf[Read[_, _]])

import org.apache.beam.sdk.Pipeline
val p = Pipeline.create()

scala> p.apply("Records from beam-input topic", records)
DEBUG Pipeline: Adding KafkaIO.Read to Pipeline#505559573
DEBUG CoderRegistry: Coder for java.lang.String: StringUtf8Coder
DEBUG CoderRegistry: Coder for java.lang.String: StringUtf8Coder
DEBUG Pipeline: Adding Read(KafkaUnboundedSource) to Pipeline#505559573
----
